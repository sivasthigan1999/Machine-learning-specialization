{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Logistic Regression with L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to implement your own logistic regression classifier with L2 regularization. You will do the following:\n",
    "\n",
    "Extract features from Amazon product reviews.\n",
    "Convert an dataframe into a NumPy array.\n",
    "Write a function to compute the derivative of log likelihood function with an L2 penalty with respect to a single coefficient.\n",
    "Implement gradient ascent with an L2 penalty.\n",
    "Empirically explore how the L2 penalty can ameliorate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('/Users/April/Downloads/amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/Users/April/Desktop/datasci_course_materials-master/assignment1/important words.json', 'r') as f: # Reads the list of most frequent words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda pf: 1 if pf >=1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2955"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(products['contains_perfect'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/April/Desktop/datasci_course_materials-master/assignment1/train index.json', 'r') as f1:\n",
    "    train_index = json.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/Users/April/Desktop/datasci_course_materials-master/assignment1/validation index.json', 'r') as f2:\n",
    "    validation_index = json.load(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = products.iloc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_data = products.iloc[validation_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Convert data frame to multi-dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    features_frame = dataframe[features]\n",
    "    feature_matrix = features_frame.as_matrix()\n",
    "    label_sarray = dataframe[label]\n",
    "    label_array = label_sarray.as_matrix()\n",
    "    return(feature_matrix, label_array) #Why do we need transpose it to matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/April/anaconda/envs/dato-env/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building on logistic regression with no L2 penalty assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    # YOUR CODE HERE\n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    # YOUR CODE HERE\n",
    "    predictions = 1/(1+np.exp(-score))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Adding L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us now work on extending logistic regression with an L2 penalty. As discussed in the lectures, the L2 regularization is particularly useful in preventing overfitting. In this assignment, we will explore L2 regularization in detail.\n",
    "\n",
    "Recall from lecture and the previous assignment that for logistic regression without an L2 penalty, the derivative of the log-likelihood function is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that computes the derivative of log likelihood with respect to a single coefficient w_j. Unlike its counterpart in the last assignment, the function accepts five parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function should do the following:\n",
    "\n",
    "Take the five parameters as above.\n",
    "Compute the dot product of errors and feature and save the result to derivative.\n",
    "If feature_is_constant is False, subtract the L2 penalty term from derivative. Otherwise, do nothing.\n",
    "Return derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant): \n",
    "   \n",
    "    # Compute the dot product of errors and feature\n",
    "    ## YOUR CODE HERE\n",
    "    derivative = np.dot(errors, feature) \n",
    "\n",
    "    # add L2 penalty term for any feature that isn't the intercept.\n",
    "    if not feature_is_constant: \n",
    "        ## YOUR CODE HERE\n",
    "        derivative = np.dot(errors, feature) - 2*l2_penalty*coefficient\n",
    "    \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of the gradient descent algorithm, we write a function for computing log likelihood (which we recall from the last assignment was a topic detailed in an advanced optional video, and used here for its numerical stability), which is given by the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: Does the term with L2 regularization increase or decrease ℓℓ(w)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decrease because l2_penalty*np.sum(coefficients[1:]**2) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he logistic regression function looks almost like the one in the last assignment, with a minor modification to account for the L2 penalty.\n",
    "\n",
    "Write a function logistic_regression_with_L2 to fit a logistic regression model under L2 regularization.\n",
    "\n",
    "The function accepts the following parameters:\n",
    "\n",
    "feature_matrix: 2D array of features\n",
    "sentiment: 1D array of class labels\n",
    "initial_coefficients: 1D array containing initial values of coefficients\n",
    "step_size: a parameter controlling the size of the gradient steps\n",
    "l2_penalty: the L2 penalty constant λ\n",
    "max_iter: number of iterations to run gradient ascent\n",
    "The function returns the last set of coefficients after performing gradient ascent.\n",
    "\n",
    "The function carries out the following steps:\n",
    "\n",
    "Initialize vector coefficients to initial_coefficients.\n",
    "Predict the class probability P(y_i = +1 | x_i,w) using your predict_probability function and save it to variable predictions.\n",
    "Compute indicator value for (y_i = +1) by comparing sentiment against +1. Save it to variable indicator.\n",
    "Compute the errors as difference between indicator and predictions. Save the errors to variable errors.\n",
    "For each j-th coefficient, compute the per-coefficient derivative by calling feature_derivative_L2 with the j-th column of feature_matrix. Don't forget to supply the L2 penalty. Then increment the j-th coefficient by (step_size*derivative).\n",
    "Once in a while, insert code to print out the log likelihood.\n",
    "Repeat steps 2-6 for max_iter times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, \n",
    "  step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in xrange(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = feature_derivative_with_L2(\n",
    "                errors, feature_matrix[:,j], coefficients[j], l2_penalty, is_intercept)\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j] = step_size * derivative\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Explore effects of L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written up all the pieces needed for an L2 solver with logistic regression, let's explore the benefits of using L2 regularization while analyzing sentiment for product reviews. As iterations pass, the log likelihood should increase.\n",
    "\n",
    "Let us train models with increasing amounts of regularization, starting with no L2 penalty, which is equivalent to our previous logistic regression implementation. Train 6 models with L2 penalty values 0, 4, 10, 1e2, 1e3, and 1e5. Use the following values for the other parameters:\n",
    "\n",
    "feature_matrix = feature_matrix_train extracted in #7\n",
    "sentiment = sentiment_train extracted in #7\n",
    "initial_coefficients = a 194-dimensional vector filled with zeros\n",
    "step_size = 5e-6\n",
    "max_iter = 501\n",
    "Save the 6 sets of coefficients as coefficients_0_penalty, coefficients_4_penalty, coefficients_10_penalty, coefficients_1e2_penalty, coefficients_1e3_penalty, and coefficients_1e5_penalty respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L2_penalty_list = [0, 4, 10, 1e2, 1e3, 1e5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix = feature_matrix_train\n",
    "sentiment = sentiment_train \n",
    "initial_coefficients = np.zeros(194)\n",
    "step_size = 5e-6\n",
    "max_iter = 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.39138303\n",
      "iteration   1: log likelihood of observed labels = -29183.07101158\n",
      "iteration   2: log likelihood of observed labels = -29182.87850538\n",
      "iteration   3: log likelihood of observed labels = -29182.90844738\n",
      "iteration   4: log likelihood of observed labels = -29182.90188394\n",
      "iteration   5: log likelihood of observed labels = -29182.90339394\n",
      "iteration   6: log likelihood of observed labels = -29182.90304383\n",
      "iteration   7: log likelihood of observed labels = -29182.90312511\n",
      "iteration   8: log likelihood of observed labels = -29182.90310623\n",
      "iteration   9: log likelihood of observed labels = -29182.90311061\n",
      "iteration  10: log likelihood of observed labels = -29182.90310960\n",
      "iteration  11: log likelihood of observed labels = -29182.90310983\n",
      "iteration  12: log likelihood of observed labels = -29182.90310978\n",
      "iteration  13: log likelihood of observed labels = -29182.90310979\n",
      "iteration  14: log likelihood of observed labels = -29182.90310979\n",
      "iteration  15: log likelihood of observed labels = -29182.90310979\n",
      "iteration  20: log likelihood of observed labels = -29182.90310979\n",
      "iteration  30: log likelihood of observed labels = -29182.90310979\n",
      "iteration  40: log likelihood of observed labels = -29182.90310979\n",
      "iteration  50: log likelihood of observed labels = -29182.90310979\n",
      "iteration  60: log likelihood of observed labels = -29182.90310979\n",
      "iteration  70: log likelihood of observed labels = -29182.90310979\n",
      "iteration  80: log likelihood of observed labels = -29182.90310979\n",
      "iteration  90: log likelihood of observed labels = -29182.90310979\n",
      "iteration 100: log likelihood of observed labels = -29182.90310979\n",
      "iteration 200: log likelihood of observed labels = -29182.90310979\n",
      "iteration 300: log likelihood of observed labels = -29182.90310979\n",
      "iteration 400: log likelihood of observed labels = -29182.90310979\n",
      "iteration 500: log likelihood of observed labels = -29182.90310979\n",
      "iteration   0: log likelihood of observed labels = -29179.39508175\n",
      "iteration   1: log likelihood of observed labels = -29183.08180871\n",
      "iteration   2: log likelihood of observed labels = -29182.88901196\n",
      "iteration   3: log likelihood of observed labels = -29182.91897600\n",
      "iteration   4: log likelihood of observed labels = -29182.91240811\n",
      "iteration   5: log likelihood of observed labels = -29182.91391930\n",
      "iteration   6: log likelihood of observed labels = -29182.91356886\n",
      "iteration   7: log likelihood of observed labels = -29182.91365023\n",
      "iteration   8: log likelihood of observed labels = -29182.91363133\n",
      "iteration   9: log likelihood of observed labels = -29182.91363572\n",
      "iteration  10: log likelihood of observed labels = -29182.91363470\n",
      "iteration  11: log likelihood of observed labels = -29182.91363494\n",
      "iteration  12: log likelihood of observed labels = -29182.91363488\n",
      "iteration  13: log likelihood of observed labels = -29182.91363490\n",
      "iteration  14: log likelihood of observed labels = -29182.91363489\n",
      "iteration  15: log likelihood of observed labels = -29182.91363489\n",
      "iteration  20: log likelihood of observed labels = -29182.91363489\n",
      "iteration  30: log likelihood of observed labels = -29182.91363489\n",
      "iteration  40: log likelihood of observed labels = -29182.91363489\n",
      "iteration  50: log likelihood of observed labels = -29182.91363489\n",
      "iteration  60: log likelihood of observed labels = -29182.91363489\n",
      "iteration  70: log likelihood of observed labels = -29182.91363489\n",
      "iteration  80: log likelihood of observed labels = -29182.91363489\n",
      "iteration  90: log likelihood of observed labels = -29182.91363489\n",
      "iteration 100: log likelihood of observed labels = -29182.91363489\n",
      "iteration 200: log likelihood of observed labels = -29182.91363489\n",
      "iteration 300: log likelihood of observed labels = -29182.91363489\n",
      "iteration 400: log likelihood of observed labels = -29182.91363489\n",
      "iteration 500: log likelihood of observed labels = -29182.91363489\n",
      "iteration   0: log likelihood of observed labels = -29179.40062984\n",
      "iteration   1: log likelihood of observed labels = -29183.09800333\n",
      "iteration   2: log likelihood of observed labels = -29182.90476969\n",
      "iteration   3: log likelihood of observed labels = -29182.93476687\n",
      "iteration   4: log likelihood of observed labels = -29182.92819230\n",
      "iteration   5: log likelihood of observed labels = -29182.92970529\n",
      "iteration   6: log likelihood of observed labels = -29182.92935436\n",
      "iteration   7: log likelihood of observed labels = -29182.92943586\n",
      "iteration   8: log likelihood of observed labels = -29182.92941692\n",
      "iteration   9: log likelihood of observed labels = -29182.92942132\n",
      "iteration  10: log likelihood of observed labels = -29182.92942030\n",
      "iteration  11: log likelihood of observed labels = -29182.92942054\n",
      "iteration  12: log likelihood of observed labels = -29182.92942048\n",
      "iteration  13: log likelihood of observed labels = -29182.92942049\n",
      "iteration  14: log likelihood of observed labels = -29182.92942049\n",
      "iteration  15: log likelihood of observed labels = -29182.92942049\n",
      "iteration  20: log likelihood of observed labels = -29182.92942049\n",
      "iteration  30: log likelihood of observed labels = -29182.92942049\n",
      "iteration  40: log likelihood of observed labels = -29182.92942049\n",
      "iteration  50: log likelihood of observed labels = -29182.92942049\n",
      "iteration  60: log likelihood of observed labels = -29182.92942049\n",
      "iteration  70: log likelihood of observed labels = -29182.92942049\n",
      "iteration  80: log likelihood of observed labels = -29182.92942049\n",
      "iteration  90: log likelihood of observed labels = -29182.92942049\n",
      "iteration 100: log likelihood of observed labels = -29182.92942049\n",
      "iteration 200: log likelihood of observed labels = -29182.92942049\n",
      "iteration 300: log likelihood of observed labels = -29182.92942049\n",
      "iteration 400: log likelihood of observed labels = -29182.92942049\n",
      "iteration 500: log likelihood of observed labels = -29182.92942049\n",
      "iteration   0: log likelihood of observed labels = -29179.48385120\n",
      "iteration   1: log likelihood of observed labels = -29183.34076800\n",
      "iteration   2: log likelihood of observed labels = -29183.14083061\n",
      "iteration   3: log likelihood of observed labels = -29183.17133414\n",
      "iteration   4: log likelihood of observed labels = -29183.16465845\n",
      "iteration   5: log likelihood of observed labels = -29183.16619860\n",
      "iteration   6: log likelihood of observed labels = -29183.16584022\n",
      "iteration   7: log likelihood of observed labels = -29183.16592373\n",
      "iteration   8: log likelihood of observed labels = -29183.16590427\n",
      "iteration   9: log likelihood of observed labels = -29183.16590880\n",
      "iteration  10: log likelihood of observed labels = -29183.16590775\n",
      "iteration  11: log likelihood of observed labels = -29183.16590799\n",
      "iteration  12: log likelihood of observed labels = -29183.16590793\n",
      "iteration  13: log likelihood of observed labels = -29183.16590795\n",
      "iteration  14: log likelihood of observed labels = -29183.16590794\n",
      "iteration  15: log likelihood of observed labels = -29183.16590795\n",
      "iteration  20: log likelihood of observed labels = -29183.16590795\n",
      "iteration  30: log likelihood of observed labels = -29183.16590795\n",
      "iteration  40: log likelihood of observed labels = -29183.16590795\n",
      "iteration  50: log likelihood of observed labels = -29183.16590795\n",
      "iteration  60: log likelihood of observed labels = -29183.16590795\n",
      "iteration  70: log likelihood of observed labels = -29183.16590795\n",
      "iteration  80: log likelihood of observed labels = -29183.16590795\n",
      "iteration  90: log likelihood of observed labels = -29183.16590795\n",
      "iteration 100: log likelihood of observed labels = -29183.16590795\n",
      "iteration 200: log likelihood of observed labels = -29183.16590795\n",
      "iteration 300: log likelihood of observed labels = -29183.16590795\n",
      "iteration 400: log likelihood of observed labels = -29183.16590795\n",
      "iteration 500: log likelihood of observed labels = -29183.16590795\n",
      "iteration   0: log likelihood of observed labels = -29180.31606471\n",
      "iteration   1: log likelihood of observed labels = -29185.75255029\n",
      "iteration   2: log likelihood of observed labels = -29185.47020552\n",
      "iteration   3: log likelihood of observed labels = -29185.50688854\n",
      "iteration   4: log likelihood of observed labels = -29185.49909176\n",
      "iteration   5: log likelihood of observed labels = -29185.50092862\n",
      "iteration   6: log likelihood of observed labels = -29185.50048789\n",
      "iteration   7: log likelihood of observed labels = -29185.50059397\n",
      "iteration   8: log likelihood of observed labels = -29185.50056842\n",
      "iteration   9: log likelihood of observed labels = -29185.50057458\n",
      "iteration  10: log likelihood of observed labels = -29185.50057309\n",
      "iteration  11: log likelihood of observed labels = -29185.50057345\n",
      "iteration  12: log likelihood of observed labels = -29185.50057336\n",
      "iteration  13: log likelihood of observed labels = -29185.50057338\n",
      "iteration  14: log likelihood of observed labels = -29185.50057338\n",
      "iteration  15: log likelihood of observed labels = -29185.50057338\n",
      "iteration  20: log likelihood of observed labels = -29185.50057338\n",
      "iteration  30: log likelihood of observed labels = -29185.50057338\n",
      "iteration  40: log likelihood of observed labels = -29185.50057338\n",
      "iteration  50: log likelihood of observed labels = -29185.50057338\n",
      "iteration  60: log likelihood of observed labels = -29185.50057338\n",
      "iteration  70: log likelihood of observed labels = -29185.50057338\n",
      "iteration  80: log likelihood of observed labels = -29185.50057338\n",
      "iteration  90: log likelihood of observed labels = -29185.50057338\n",
      "iteration 100: log likelihood of observed labels = -29185.50057338\n",
      "iteration 200: log likelihood of observed labels = -29185.50057338\n",
      "iteration 300: log likelihood of observed labels = -29185.50057338\n",
      "iteration 400: log likelihood of observed labels = -29185.50057338\n",
      "iteration 500: log likelihood of observed labels = -29185.50057338\n",
      "iteration   0: log likelihood of observed labels = -29271.85955115\n",
      "iteration   1: log likelihood of observed labels = -29366.39743365\n",
      "iteration   2: log likelihood of observed labels = -29272.28662274\n",
      "iteration   3: log likelihood of observed labels = -29371.36631985\n",
      "iteration   4: log likelihood of observed labels = -29273.41455823\n",
      "iteration   5: log likelihood of observed labels = -29377.95813188\n",
      "iteration   6: log likelihood of observed labels = -29276.07066862\n",
      "iteration   7: log likelihood of observed labels = -29387.59692438\n",
      "iteration   8: log likelihood of observed labels = -29282.04882174\n",
      "iteration   9: log likelihood of observed labels = -29403.21951230\n",
      "iteration  10: log likelihood of observed labels = -29295.22551596\n",
      "iteration  11: log likelihood of observed labels = -29430.93992407\n",
      "iteration  12: log likelihood of observed labels = -29323.92452082\n",
      "iteration  13: log likelihood of observed labels = -29483.54002640\n",
      "iteration  14: log likelihood of observed labels = -29385.88501967\n",
      "iteration  15: log likelihood of observed labels = -29587.70098514\n",
      "iteration  20: log likelihood of observed labels = -30383.61253347\n",
      "iteration  30: log likelihood of observed labels = -58811.30162728\n",
      "iteration  40: log likelihood of observed labels = -267103.55874604\n",
      "iteration  50: log likelihood of observed labels = -830379.86254776\n",
      "iteration  60: log likelihood of observed labels = -1804461.93360455\n",
      "iteration  70: log likelihood of observed labels = -3199572.56513649\n",
      "iteration  80: log likelihood of observed labels = -5017057.39174467\n",
      "iteration  90: log likelihood of observed labels = -7256607.10695471\n",
      "iteration 100: log likelihood of observed labels = -9917707.36313455\n",
      "iteration 200: log likelihood of observed labels = -inf\n",
      "iteration 300: log likelihood of observed labels = -inf\n",
      "iteration 400: log likelihood of observed labels = -inf\n",
      "iteration 500: log likelihood of observed labels = -inf\n"
     ]
    }
   ],
   "source": [
    "coefficients_list = [logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, \n",
    "  step_size, l2_penalty, max_iter) for l2_penalty in L2_penalty_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.26218044e-04,   1.94927276e-03,  -1.64183528e-03,\n",
       "         9.05744405e-03,   9.11075879e-03,   5.51855484e-04,\n",
       "        -7.96348801e-03,  -7.01144157e-04,   8.87843938e-03,\n",
       "         6.02880307e-03,   7.10408055e-04,   2.83324647e-03,\n",
       "         4.02453428e-03,  -4.46251528e-03,   1.99321446e-03,\n",
       "        -2.55964933e-04,   1.62609911e-03,  -2.71417899e-03,\n",
       "        -1.89939300e-03,  -6.10049409e-03,   3.33719121e-04,\n",
       "         2.46130817e-03,  -7.63639479e-04,   6.19003193e-03,\n",
       "         1.37874615e-03,  -2.59940964e-05,  -8.87149256e-05,\n",
       "         1.81610581e-03,   1.03213972e-03,  -3.91408067e-03,\n",
       "         8.79186956e-05,   1.88546698e-03,  -1.94173840e-03,\n",
       "        -4.29341996e-03,   4.01926224e-03,   2.45245021e-03,\n",
       "         3.53484058e-04,  -1.63170875e-03,  -9.14470443e-05,\n",
       "        -1.54234210e-03,  -7.65713157e-04,   1.06534141e-03,\n",
       "         5.10244008e-04,   3.34780389e-04,  -3.56814277e-04,\n",
       "         1.11215457e-03,  -8.09968032e-05,  -2.90423802e-03,\n",
       "         2.36275138e-03,   2.05523158e-04,  -2.54953672e-03,\n",
       "         1.19758077e-03,   1.53871141e-03,  -9.93517014e-04,\n",
       "        -8.36754025e-04,   8.20715002e-04,   1.41175021e-03,\n",
       "        -5.02091903e-04,   8.92304365e-04,   8.11930148e-04,\n",
       "         4.98885437e-04,  -7.58378216e-04,   8.60437038e-04,\n",
       "         1.08314732e-03,   2.66321939e-04,   8.26787897e-04,\n",
       "         1.56615541e-03,  -2.57495466e-03,  -1.05880810e-03,\n",
       "        -1.81766412e-03,  -2.09773083e-03,   8.27201819e-04,\n",
       "         7.85352620e-04,  -9.01342023e-04,   3.43592562e-04,\n",
       "         6.80419596e-04,   1.99468892e-03,  -3.93412539e-05,\n",
       "        -4.72387732e-03,  -7.61743866e-04,  -1.03318531e-03,\n",
       "         1.01674754e-03,  -2.37908236e-04,   2.18289034e-03,\n",
       "         1.81871279e-03,   3.08041141e-04,   2.76050765e-04,\n",
       "         1.06317382e-03,   1.53419270e-03,   4.04283520e-04,\n",
       "         2.91411175e-04,   2.32707340e-03,  -7.22041853e-04,\n",
       "         1.89254799e-04,  -1.54976815e-03,   1.24659781e-03,\n",
       "        -4.73287848e-05,  -5.44344982e-03,  -3.39312108e-03,\n",
       "        -2.62890682e-03,  -3.04500144e-03,  -1.80305417e-03,\n",
       "        -2.42454846e-03,  -1.86869039e-03,  -2.12306765e-03,\n",
       "        -7.48597787e-04,  -3.98958800e-03,  -2.12205398e-03,\n",
       "        -1.78819669e-03,  -1.57254698e-03,  -1.35820840e-03,\n",
       "        -7.72774674e-04,  -1.39763472e-03,  -3.32535598e-03,\n",
       "        -3.70907755e-03,  -3.07638085e-04,  -7.20124931e-04,\n",
       "        -8.50518594e-04,  -5.78029353e-04,  -9.42270132e-04,\n",
       "        -6.08197872e-04,  -9.09981607e-04,  -1.58638464e-03,\n",
       "        -1.81490718e-03,  -2.58831524e-04,  -1.06525277e-03,\n",
       "        -1.82159769e-04,  -2.83868100e-04,  -2.41195391e-04,\n",
       "        -6.96563057e-04,  -1.28955228e-03,  -1.01140548e-03,\n",
       "        -1.19973890e-03,  -8.09974780e-04,  -2.04820822e-03,\n",
       "        -1.30555072e-03,  -3.44988017e-04,  -1.04656099e-03,\n",
       "        -7.00086901e-04,  -1.36696026e-03,  -1.74229512e-03,\n",
       "        -9.60545551e-04,  -1.46608079e-04,  -4.97744668e-06,\n",
       "         1.10254012e-04,   2.91643949e-04,  -1.04236595e-03,\n",
       "         1.90592250e-04,  -1.11068681e-03,  -6.02631789e-04,\n",
       "        -6.48528071e-04,  -2.47518543e-04,  -4.88655308e-04,\n",
       "         1.79957890e-04,   1.46106026e-05,  -4.65560727e-04,\n",
       "        -1.46592439e-03,  -1.57408520e-03,   2.20152192e-04,\n",
       "        -1.42883748e-03,  -1.40470251e-03,  -5.56152762e-04,\n",
       "         4.15283332e-04,  -6.28572130e-04,  -8.57353226e-04,\n",
       "        -1.03433352e-03,  -7.55867269e-04,  -1.70544759e-03,\n",
       "        -4.04222559e-04,  -2.21150356e-03,  -9.35134815e-04,\n",
       "        -7.52545308e-04,  -2.16823868e-03,   2.78965849e-04,\n",
       "        -1.29299240e-03,  -9.08846674e-04,  -1.61225318e-03,\n",
       "        -4.82275585e-04,  -9.30005716e-04,  -8.54803412e-04,\n",
       "        -9.89078133e-04,  -1.65953752e-03,  -4.27271346e-05,\n",
       "        -1.55418112e-03,   3.50336287e-04,  -1.13476113e-03,\n",
       "        -1.40700607e-03,   4.88846638e-04,  -7.73708767e-04,\n",
       "         3.83523461e-04,   1.75988726e-05,  -9.19065721e-04,\n",
       "        -1.05169619e-04,  -9.65233300e-04])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficients_name = ['coefficients_0_penalty', 'coefficients_4_penalty', 'coefficients_10_penalty', 'coefficients_1e2_penalty', \n",
    "                     'coefficients_1e3_penalty', 'coefficients_1e5_penalty' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coefficients_dict = dict(zip(coefficients_name, coefficients_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coefficients_0_penalty': array([  5.26235043e-04,   1.94947479e-03,  -1.64197820e-03,\n",
       "          9.05834108e-03,   9.11165871e-03,   5.51918608e-04,\n",
       "         -7.96424741e-03,  -7.01202770e-04,   8.87931807e-03,\n",
       "          6.02940198e-03,   7.10478734e-04,   2.83352951e-03,\n",
       "          4.02493663e-03,  -4.46294134e-03,   1.99341540e-03,\n",
       "         -2.55983813e-04,   1.62626403e-03,  -2.71443539e-03,\n",
       "         -1.89957480e-03,  -6.10108191e-03,   3.33757847e-04,\n",
       "          2.46155464e-03,  -7.63708515e-04,   6.19064587e-03,\n",
       "          1.37888065e-03,  -2.59915516e-05,  -8.87193142e-05,\n",
       "          1.81628560e-03,   1.03224705e-03,  -3.91445818e-03,\n",
       "          8.79329071e-05,   1.88565785e-03,  -1.94192278e-03,\n",
       "         -4.29383634e-03,   4.01966219e-03,   2.45269558e-03,\n",
       "          3.53521569e-04,  -1.63186449e-03,  -9.14513400e-05,\n",
       "         -1.54248934e-03,  -7.65786397e-04,   1.06544986e-03,\n",
       "          5.10296788e-04,   3.34816733e-04,  -3.56848284e-04,\n",
       "          1.11226765e-03,  -8.10006223e-05,  -2.90451720e-03,\n",
       "          2.36298653e-03,   2.05547421e-04,  -2.54978453e-03,\n",
       "          1.19770326e-03,   1.53886636e-03,  -9.93611777e-04,\n",
       "         -8.36833041e-04,   8.20799170e-04,   1.41189174e-03,\n",
       "         -5.02139817e-04,   8.92396267e-04,   8.12012150e-04,\n",
       "          4.98936541e-04,  -7.58449029e-04,   8.60525354e-04,\n",
       "          1.08325581e-03,   2.66352314e-04,   8.26872398e-04,\n",
       "          1.56631137e-03,  -2.57520472e-03,  -1.05890976e-03,\n",
       "         -1.81783934e-03,  -2.09793428e-03,   8.27287048e-04,\n",
       "          7.85432869e-04,  -9.01426660e-04,   3.43630073e-04,\n",
       "          6.80487804e-04,   1.99488888e-03,  -3.93406721e-05,\n",
       "         -4.72433796e-03,  -7.61816376e-04,  -1.03328422e-03,\n",
       "          1.01685074e-03,  -2.37928893e-04,   2.18310873e-03,\n",
       "          1.81889404e-03,   3.08072062e-04,   2.76081485e-04,\n",
       "          1.06328065e-03,   1.53434674e-03,   4.04327930e-04,\n",
       "          2.91442782e-04,   2.32730469e-03,  -7.22109591e-04,\n",
       "          1.89276489e-04,  -1.54991767e-03,   1.24672243e-03,\n",
       "         -4.73299613e-05,  -5.44398504e-03,  -3.39344626e-03,\n",
       "         -2.62916292e-03,  -3.04529949e-03,  -1.80322906e-03,\n",
       "         -2.42478618e-03,  -1.86887310e-03,  -2.12327498e-03,\n",
       "         -7.48671421e-04,  -3.98998292e-03,  -2.12226041e-03,\n",
       "         -1.78837105e-03,  -1.57269903e-03,  -1.35833388e-03,\n",
       "         -7.72847022e-04,  -1.39777160e-03,  -3.32568426e-03,\n",
       "         -3.70944316e-03,  -3.07668170e-04,  -7.20193394e-04,\n",
       "         -8.50600583e-04,  -5.78083271e-04,  -9.42360070e-04,\n",
       "         -6.08254736e-04,  -9.10069221e-04,  -1.58653884e-03,\n",
       "         -1.81508494e-03,  -2.58854211e-04,  -1.06535571e-03,\n",
       "         -1.82176597e-04,  -2.83893542e-04,  -2.41217467e-04,\n",
       "         -6.96628713e-04,  -1.28967825e-03,  -1.01150406e-03,\n",
       "         -1.19985590e-03,  -8.10052674e-04,  -2.04841054e-03,\n",
       "         -1.30567787e-03,  -3.45019656e-04,  -1.04666301e-03,\n",
       "         -7.00154205e-04,  -1.36709135e-03,  -1.74246633e-03,\n",
       "         -9.60638746e-04,  -1.46620670e-04,  -4.97532365e-06,\n",
       "          1.10266224e-04,   2.91675396e-04,  -1.04246795e-03,\n",
       "          1.90612582e-04,  -1.11079429e-03,  -6.02688935e-04,\n",
       "         -6.48590126e-04,  -2.47541578e-04,  -4.88702088e-04,\n",
       "          1.79977606e-04,   1.46128098e-05,  -4.65604187e-04,\n",
       "         -1.46606785e-03,  -1.57423949e-03,   2.20175662e-04,\n",
       "         -1.42897706e-03,  -1.40484097e-03,  -5.56206492e-04,\n",
       "          4.15327018e-04,  -6.28631913e-04,  -8.57436246e-04,\n",
       "         -1.03443442e-03,  -7.55940239e-04,  -1.70561444e-03,\n",
       "         -4.04260689e-04,  -2.21172227e-03,  -9.35226520e-04,\n",
       "         -7.52617626e-04,  -2.16845324e-03,   2.78994849e-04,\n",
       "         -1.29311968e-03,  -9.08935422e-04,  -1.61241256e-03,\n",
       "         -4.82322331e-04,  -9.30095519e-04,  -8.54886167e-04,\n",
       "         -9.89173764e-04,  -1.65970017e-03,  -4.27299535e-05,\n",
       "         -1.55433227e-03,   3.50372791e-04,  -1.13487204e-03,\n",
       "         -1.40714420e-03,   4.88896923e-04,  -7.73783980e-04,\n",
       "          3.83562691e-04,   1.76006386e-05,  -9.19155970e-04,\n",
       "         -1.05178680e-04,  -9.65327833e-04]),\n",
       " 'coefficients_10_penalty': array([  5.26218044e-04,   1.94927276e-03,  -1.64183528e-03,\n",
       "          9.05744405e-03,   9.11075879e-03,   5.51855484e-04,\n",
       "         -7.96348801e-03,  -7.01144157e-04,   8.87843938e-03,\n",
       "          6.02880307e-03,   7.10408055e-04,   2.83324647e-03,\n",
       "          4.02453428e-03,  -4.46251528e-03,   1.99321446e-03,\n",
       "         -2.55964933e-04,   1.62609911e-03,  -2.71417899e-03,\n",
       "         -1.89939300e-03,  -6.10049409e-03,   3.33719121e-04,\n",
       "          2.46130817e-03,  -7.63639479e-04,   6.19003193e-03,\n",
       "          1.37874615e-03,  -2.59940964e-05,  -8.87149256e-05,\n",
       "          1.81610581e-03,   1.03213972e-03,  -3.91408067e-03,\n",
       "          8.79186956e-05,   1.88546698e-03,  -1.94173840e-03,\n",
       "         -4.29341996e-03,   4.01926224e-03,   2.45245021e-03,\n",
       "          3.53484058e-04,  -1.63170875e-03,  -9.14470443e-05,\n",
       "         -1.54234210e-03,  -7.65713157e-04,   1.06534141e-03,\n",
       "          5.10244008e-04,   3.34780389e-04,  -3.56814277e-04,\n",
       "          1.11215457e-03,  -8.09968032e-05,  -2.90423802e-03,\n",
       "          2.36275138e-03,   2.05523158e-04,  -2.54953672e-03,\n",
       "          1.19758077e-03,   1.53871141e-03,  -9.93517014e-04,\n",
       "         -8.36754025e-04,   8.20715002e-04,   1.41175021e-03,\n",
       "         -5.02091903e-04,   8.92304365e-04,   8.11930148e-04,\n",
       "          4.98885437e-04,  -7.58378216e-04,   8.60437038e-04,\n",
       "          1.08314732e-03,   2.66321939e-04,   8.26787897e-04,\n",
       "          1.56615541e-03,  -2.57495466e-03,  -1.05880810e-03,\n",
       "         -1.81766412e-03,  -2.09773083e-03,   8.27201819e-04,\n",
       "          7.85352620e-04,  -9.01342023e-04,   3.43592562e-04,\n",
       "          6.80419596e-04,   1.99468892e-03,  -3.93412539e-05,\n",
       "         -4.72387732e-03,  -7.61743866e-04,  -1.03318531e-03,\n",
       "          1.01674754e-03,  -2.37908236e-04,   2.18289034e-03,\n",
       "          1.81871279e-03,   3.08041141e-04,   2.76050765e-04,\n",
       "          1.06317382e-03,   1.53419270e-03,   4.04283520e-04,\n",
       "          2.91411175e-04,   2.32707340e-03,  -7.22041853e-04,\n",
       "          1.89254799e-04,  -1.54976815e-03,   1.24659781e-03,\n",
       "         -4.73287848e-05,  -5.44344982e-03,  -3.39312108e-03,\n",
       "         -2.62890682e-03,  -3.04500144e-03,  -1.80305417e-03,\n",
       "         -2.42454846e-03,  -1.86869039e-03,  -2.12306765e-03,\n",
       "         -7.48597787e-04,  -3.98958800e-03,  -2.12205398e-03,\n",
       "         -1.78819669e-03,  -1.57254698e-03,  -1.35820840e-03,\n",
       "         -7.72774674e-04,  -1.39763472e-03,  -3.32535598e-03,\n",
       "         -3.70907755e-03,  -3.07638085e-04,  -7.20124931e-04,\n",
       "         -8.50518594e-04,  -5.78029353e-04,  -9.42270132e-04,\n",
       "         -6.08197872e-04,  -9.09981607e-04,  -1.58638464e-03,\n",
       "         -1.81490718e-03,  -2.58831524e-04,  -1.06525277e-03,\n",
       "         -1.82159769e-04,  -2.83868100e-04,  -2.41195391e-04,\n",
       "         -6.96563057e-04,  -1.28955228e-03,  -1.01140548e-03,\n",
       "         -1.19973890e-03,  -8.09974780e-04,  -2.04820822e-03,\n",
       "         -1.30555072e-03,  -3.44988017e-04,  -1.04656099e-03,\n",
       "         -7.00086901e-04,  -1.36696026e-03,  -1.74229512e-03,\n",
       "         -9.60545551e-04,  -1.46608079e-04,  -4.97744668e-06,\n",
       "          1.10254012e-04,   2.91643949e-04,  -1.04236595e-03,\n",
       "          1.90592250e-04,  -1.11068681e-03,  -6.02631789e-04,\n",
       "         -6.48528071e-04,  -2.47518543e-04,  -4.88655308e-04,\n",
       "          1.79957890e-04,   1.46106026e-05,  -4.65560727e-04,\n",
       "         -1.46592439e-03,  -1.57408520e-03,   2.20152192e-04,\n",
       "         -1.42883748e-03,  -1.40470251e-03,  -5.56152762e-04,\n",
       "          4.15283332e-04,  -6.28572130e-04,  -8.57353226e-04,\n",
       "         -1.03433352e-03,  -7.55867269e-04,  -1.70544759e-03,\n",
       "         -4.04222559e-04,  -2.21150356e-03,  -9.35134815e-04,\n",
       "         -7.52545308e-04,  -2.16823868e-03,   2.78965849e-04,\n",
       "         -1.29299240e-03,  -9.08846674e-04,  -1.61225318e-03,\n",
       "         -4.82275585e-04,  -9.30005716e-04,  -8.54803412e-04,\n",
       "         -9.89078133e-04,  -1.65953752e-03,  -4.27271346e-05,\n",
       "         -1.55418112e-03,   3.50336287e-04,  -1.13476113e-03,\n",
       "         -1.40700607e-03,   4.88846638e-04,  -7.73708767e-04,\n",
       "          3.83523461e-04,   1.75988726e-05,  -9.19065721e-04,\n",
       "         -1.05169619e-04,  -9.65233300e-04]),\n",
       " 'coefficients_1e2_penalty': array([  5.26065177e-04,   1.94745641e-03,  -1.64055008e-03,\n",
       "          9.04937879e-03,   9.10266749e-03,   5.51287990e-04,\n",
       "         -7.95665992e-03,  -7.00617076e-04,   8.87053894e-03,\n",
       "          6.02341822e-03,   7.09772566e-04,   2.83070170e-03,\n",
       "          4.02091669e-03,  -4.45868433e-03,   1.99140785e-03,\n",
       "         -2.55795135e-04,   1.62461631e-03,  -2.71187363e-03,\n",
       "         -1.89775839e-03,  -6.09520887e-03,   3.33370975e-04,\n",
       "          2.45909212e-03,  -7.63018717e-04,   6.18451193e-03,\n",
       "          1.37753677e-03,  -2.60169391e-05,  -8.86754361e-05,\n",
       "          1.81448925e-03,   1.03117475e-03,  -3.91068631e-03,\n",
       "          8.77909604e-05,   1.88375088e-03,  -1.94008054e-03,\n",
       "         -4.28967619e-03,   4.01566624e-03,   2.45024413e-03,\n",
       "          3.53146811e-04,  -1.63030836e-03,  -9.14083851e-05,\n",
       "         -1.54101819e-03,  -7.65054636e-04,   1.06436636e-03,\n",
       "          5.09769475e-04,   3.34453646e-04,  -3.56508514e-04,\n",
       "          1.11113796e-03,  -8.09624331e-05,  -2.90172783e-03,\n",
       "          2.36063719e-03,   2.05305040e-04,  -2.54730863e-03,\n",
       "          1.19647942e-03,   1.53731825e-03,  -9.92664969e-04,\n",
       "         -8.36043557e-04,   8.19958266e-04,   1.41047773e-03,\n",
       "         -5.01661088e-04,   8.91478089e-04,   8.11192875e-04,\n",
       "          4.98425973e-04,  -7.57741500e-04,   8.59643011e-04,\n",
       "          1.08217187e-03,   2.66048864e-04,   8.26028163e-04,\n",
       "          1.56475318e-03,  -2.57270635e-03,  -1.05789401e-03,\n",
       "         -1.81608865e-03,  -2.09590162e-03,   8.26435549e-04,\n",
       "          7.84631105e-04,  -9.00581005e-04,   3.43255321e-04,\n",
       "          6.79806341e-04,   1.99289114e-03,  -3.93464492e-05,\n",
       "         -4.71973558e-03,  -7.61091896e-04,  -1.03229599e-03,\n",
       "          1.01581964e-03,  -2.37722486e-04,   2.18092687e-03,\n",
       "          1.81708310e-03,   3.07763127e-04,   2.75774586e-04,\n",
       "          1.06221322e-03,   1.53280775e-03,   4.03884267e-04,\n",
       "          2.91127014e-04,   2.32499394e-03,  -7.21432780e-04,\n",
       "          1.89059803e-04,  -1.54842381e-03,   1.24547733e-03,\n",
       "         -4.73181788e-05,  -5.43863754e-03,  -3.39019729e-03,\n",
       "         -2.62660419e-03,  -3.04232162e-03,  -1.80148172e-03,\n",
       "         -2.42241111e-03,  -1.86704764e-03,  -2.12120353e-03,\n",
       "         -7.47935735e-04,  -3.98603727e-03,  -2.12019792e-03,\n",
       "         -1.78662892e-03,  -1.57117978e-03,  -1.35708018e-03,\n",
       "         -7.72124152e-04,  -1.39640394e-03,  -3.32240440e-03,\n",
       "         -3.70579028e-03,  -3.07367587e-04,  -7.19509347e-04,\n",
       "         -8.49781407e-04,  -5.77544540e-04,  -9.41461464e-04,\n",
       "         -6.07686580e-04,  -9.09193842e-04,  -1.58499819e-03,\n",
       "         -1.81330892e-03,  -2.58627526e-04,  -1.06432718e-03,\n",
       "         -1.82008459e-04,  -2.83639328e-04,  -2.40996891e-04,\n",
       "         -6.95972714e-04,  -1.28841965e-03,  -1.01051905e-03,\n",
       "         -1.19868688e-03,  -8.09274408e-04,  -2.04638909e-03,\n",
       "         -1.30440747e-03,  -3.44703533e-04,  -1.04564368e-03,\n",
       "         -6.99481752e-04,  -1.36578158e-03,  -1.74075574e-03,\n",
       "         -9.59707617e-04,  -1.46494856e-04,  -4.99651488e-06,\n",
       "          1.10144224e-04,   2.91361227e-04,  -1.04144889e-03,\n",
       "          1.90409456e-04,  -1.10972047e-03,  -6.02117972e-04,\n",
       "         -6.47970111e-04,  -2.47311422e-04,  -4.88234686e-04,\n",
       "          1.79780628e-04,   1.45907625e-05,  -4.65169955e-04,\n",
       "         -1.46463449e-03,  -1.57269791e-03,   2.19941183e-04,\n",
       "         -1.42758250e-03,  -1.40345760e-03,  -5.55669664e-04,\n",
       "          4.14890568e-04,  -6.28034602e-04,  -8.56606777e-04,\n",
       "         -1.03342639e-03,  -7.55211174e-04,  -1.70394743e-03,\n",
       "         -4.03879711e-04,  -2.20953714e-03,  -9.34310276e-04,\n",
       "         -7.51895073e-04,  -2.16630954e-03,   2.78705117e-04,\n",
       "         -1.29184797e-03,  -9.08048717e-04,  -1.61082014e-03,\n",
       "         -4.81855286e-04,  -9.29198272e-04,  -8.54059336e-04,\n",
       "         -9.88218284e-04,  -1.65807509e-03,  -4.27017795e-05,\n",
       "         -1.55282212e-03,   3.50008089e-04,  -1.13376394e-03,\n",
       "         -1.40576410e-03,   4.88394535e-04,  -7.73032505e-04,\n",
       "          3.83170751e-04,   1.75829946e-05,  -9.18254279e-04,\n",
       "         -1.05088142e-04,  -9.64383335e-04]),\n",
       " 'coefficients_1e3_penalty': array([  5.24548533e-04,   1.92947502e-03,  -1.62780824e-03,\n",
       "          8.96950833e-03,   9.02253756e-03,   5.45674005e-04,\n",
       "         -7.88901879e-03,  -6.95389576e-04,   8.79230044e-03,\n",
       "          5.97009320e-03,   7.03478278e-04,   2.80550228e-03,\n",
       "          3.98509453e-03,  -4.42073459e-03,   1.97351933e-03,\n",
       "         -2.54109006e-04,   1.60993496e-03,  -2.68903421e-03,\n",
       "         -1.88156628e-03,  -6.04285664e-03,   3.29927584e-04,\n",
       "          2.43714869e-03,  -7.56866510e-04,   6.12984849e-03,\n",
       "          1.36555776e-03,  -2.62395410e-05,  -8.82813163e-05,\n",
       "          1.79847997e-03,   1.02162243e-03,  -3.87706461e-03,\n",
       "          8.65300687e-05,   1.86675962e-03,  -1.92365676e-03,\n",
       "         -4.25259513e-03,   3.98005674e-03,   2.42839921e-03,\n",
       "          3.49808668e-04,  -1.61643611e-03,  -9.10220264e-05,\n",
       "         -1.52790355e-03,  -7.58531609e-04,   1.05471253e-03,\n",
       "          5.05071729e-04,   3.31220185e-04,  -3.53479773e-04,\n",
       "          1.10107270e-03,  -8.06189182e-05,  -2.87686306e-03,\n",
       "          2.33970129e-03,   2.03147944e-04,  -2.52524048e-03,\n",
       "          1.18557585e-03,   1.52352392e-03,  -9.84224541e-04,\n",
       "         -8.29005019e-04,   8.12466462e-04,   1.39787765e-03,\n",
       "         -4.97393616e-04,   8.83298160e-04,   8.03892749e-04,\n",
       "          4.93877042e-04,  -7.51432928e-04,   8.51782072e-04,\n",
       "          1.07251278e-03,   2.63347587e-04,   8.18506528e-04,\n",
       "          1.55086743e-03,  -2.55043784e-03,  -1.04883946e-03,\n",
       "         -1.80048327e-03,  -2.07778380e-03,   8.18849730e-04,\n",
       "          7.77487861e-04,  -8.93041176e-04,   3.39918301e-04,\n",
       "          6.73733881e-04,   1.97508975e-03,  -3.93944541e-05,\n",
       "         -4.67871474e-03,  -7.54633288e-04,  -1.02348661e-03,\n",
       "          1.00663263e-03,  -2.35880951e-04,   2.16148444e-03,\n",
       "          1.80094517e-03,   3.05010135e-04,   2.73042146e-04,\n",
       "          1.05270166e-03,   1.51909424e-03,   3.99933940e-04,\n",
       "          2.88315031e-04,   2.30440167e-03,  -7.15398362e-04,\n",
       "          1.87131004e-04,  -1.53510793e-03,   1.23438208e-03,\n",
       "         -4.72104657e-05,  -5.39097891e-03,  -3.36123454e-03,\n",
       "         -2.60379817e-03,  -3.01578091e-03,  -1.78590701e-03,\n",
       "         -2.40124326e-03,  -1.85077793e-03,  -2.10274113e-03,\n",
       "         -7.41379178e-04,  -3.95087473e-03,  -2.10181466e-03,\n",
       "         -1.77110146e-03,  -1.55763772e-03,  -1.34590018e-03,\n",
       "         -7.65678876e-04,  -1.38421464e-03,  -3.29317440e-03,\n",
       "         -3.67323539e-03,  -3.04688580e-04,  -7.13411142e-04,\n",
       "         -8.42479468e-04,  -5.72741030e-04,  -9.33450722e-04,\n",
       "         -6.02620738e-04,  -9.01390786e-04,  -1.57126607e-03,\n",
       "         -1.79748004e-03,  -2.56605124e-04,  -1.05515923e-03,\n",
       "         -1.80509332e-04,  -2.81371831e-04,  -2.39029872e-04,\n",
       "         -6.90123968e-04,  -1.27720198e-03,  -1.00173975e-03,\n",
       "         -1.18826748e-03,  -8.02336978e-04,  -2.02837410e-03,\n",
       "         -1.29308427e-03,  -3.41884425e-04,  -1.03655841e-03,\n",
       "         -6.93487474e-04,  -1.35410578e-03,  -1.72551042e-03,\n",
       "         -9.51408201e-04,  -1.45372181e-04,  -5.18338535e-06,\n",
       "          1.09057827e-04,   2.88563365e-04,  -1.03236649e-03,\n",
       "          1.88600277e-04,  -1.10014895e-03,  -5.97027763e-04,\n",
       "         -6.42443064e-04,  -2.45259273e-04,  -4.84068144e-04,\n",
       "          1.78026629e-04,   1.43947437e-05,  -4.61298131e-04,\n",
       "         -1.45185949e-03,  -1.55895848e-03,   2.17852752e-04,\n",
       "         -1.41515311e-03,  -1.39112904e-03,  -5.50884616e-04,\n",
       "          4.11002968e-04,  -6.22709608e-04,  -8.49213348e-04,\n",
       "         -1.02444194e-03,  -7.48712511e-04,  -1.68908985e-03,\n",
       "         -4.00483100e-04,  -2.19006367e-03,  -9.26144283e-04,\n",
       "         -7.45454167e-04,  -2.14720538e-03,   2.76124049e-04,\n",
       "         -1.28051424e-03,  -9.00145703e-04,  -1.59662878e-03,\n",
       "         -4.77692453e-04,  -9.21200451e-04,  -8.46689387e-04,\n",
       "         -9.79701492e-04,  -1.64359146e-03,  -4.24496906e-05,\n",
       "         -1.53936185e-03,   3.46759282e-04,  -1.12388774e-03,\n",
       "         -1.39346411e-03,   4.83918824e-04,  -7.66334494e-04,\n",
       "          3.79678774e-04,   1.74257734e-05,  -9.10218117e-04,\n",
       "         -1.04280334e-04,  -9.55965462e-04]),\n",
       " 'coefficients_1e5_penalty': array([ -0.10435113, -17.34400225, -19.4598729 , -10.76861934,\n",
       "         -6.84596606, -13.82156696, -17.09198133, -13.09954478,\n",
       "         -7.24224085,  -9.03818729, -11.36318715,  -9.30170461,\n",
       "         -8.10261013, -12.13926882,  -6.83068298,  -8.68149536,\n",
       "         -7.47264499,  -9.48214557,  -8.51807578, -10.81745886,\n",
       "         -6.77892603,  -6.03179345,  -7.71590189,  -3.83409889,\n",
       "         -6.20211213,  -7.35022548,  -7.04602719,  -5.66502647,\n",
       "         -5.85915523,  -7.92824208,  -6.35988209,  -5.11165973,\n",
       "         -6.44863874,  -7.43223802,  -2.96754456,  -3.79358266,\n",
       "         -5.23628799,  -5.68090527,  -5.16092527,  -5.80792939,\n",
       "         -5.08977189,  -4.63446369,  -3.77828867,  -4.52309192,\n",
       "         -4.23502722,  -3.52350676,  -4.17693168,  -5.83825233,\n",
       "         -3.00164379,  -3.95924051,  -5.16181257,  -3.63634076,\n",
       "         -2.9962965 ,  -4.71578456,  -4.28887486,  -4.02688006,\n",
       "         -3.52661725,  -4.28019017,  -3.67544837,  -3.69105967,\n",
       "         -3.49387945,  -4.22550087,  -3.65139243,  -3.19660732,\n",
       "         -3.89207521,  -3.37440376,  -2.93238882,  -4.98020903,\n",
       "         -3.93427102,  -4.36276687,  -4.58181499,  -3.21946158,\n",
       "         -2.94294547,  -4.11752218,  -3.12433763,  -2.8691089 ,\n",
       "         -2.14865505,  -3.69519269,  -5.71805139,  -3.5909249 ,\n",
       "         -3.67677155,  -2.84363552,  -3.37018484,  -2.36249688,\n",
       "         -2.26166941,  -2.98644611,  -3.18191003,  -2.42153466,\n",
       "         -2.51525235,  -2.73978141,  -3.00118299,  -1.86225074,\n",
       "         -3.45015218,  -2.75124966,  -3.45430515,  -2.13151057,\n",
       "         -2.92592186,  -4.20637388,  -4.26103144,  -3.56425131,\n",
       "         -3.42297804,  -3.44032821,  -3.10893816,  -3.40765295,\n",
       "         -3.15800986,  -1.20146696,  -2.23524306,  -3.20939123,\n",
       "         -3.32040036,  -3.30338491,  -3.14449986,  -3.41443447,\n",
       "         -3.16118217,  -1.81936789,  -1.98312716,  -0.43599943,\n",
       "         -3.14655267,  -2.84029529,  -3.19995912,  -2.82640256,\n",
       "         -3.00600287,  -2.7108274 ,  -2.49053646,  -2.2357728 ,\n",
       "         -2.90157392,  -2.56202647,  -2.91906207,  -2.96803522,\n",
       "         -2.90926933,  -2.50739898,  -2.21991444,  -2.15909029,\n",
       "         -2.24937645,  -2.4915995 ,  -1.66639428,  -2.06694984,\n",
       "         -2.64328799,  -2.16884536,  -2.38180702,  -2.05450845,\n",
       "         -1.68972236,  -2.26231804,  -2.58185058,  -2.73501245,\n",
       "         -2.74281793,  -2.71537517,  -2.06160015,  -2.63187654,\n",
       "         -2.11404871,  -2.3531094 ,  -2.30612765,  -2.39698382,\n",
       "         -2.26245868,  -2.51876397,  -2.43273157,  -2.029158  ,\n",
       "         -1.7601148 ,  -1.75823591,  -2.46573921,  -1.80074822,\n",
       "         -1.77964489,  -2.17126032,  -2.51855397,  -2.1430805 ,\n",
       "         -1.97932702,  -1.88521658,  -1.96487414,  -1.41327674,\n",
       "         -2.19668003,  -1.24632891,  -1.04030054,  -2.03598801,\n",
       "         -1.35166042,  -2.42271554,  -1.65724995,  -1.62981131,\n",
       "         -1.48831092,  -2.06225362,  -1.80184873,  -1.85502295,\n",
       "         -1.7906124 ,  -1.43749969,  -2.04465729,  -1.35869451,\n",
       "         -2.26090042,  -1.56507009,  -1.44902683,  -2.10039246,\n",
       "         -1.77695965,  -1.91152928,  -0.02460233,  -1.66959263,\n",
       "         -2.01565666,  -1.50566009]),\n",
       " 'coefficients_4_penalty': array([  5.26228243e-04,   1.94939397e-03,  -1.64192103e-03,\n",
       "          9.05798225e-03,   9.11129872e-03,   5.51893357e-04,\n",
       "         -7.96394364e-03,  -7.01179324e-04,   8.87896657e-03,\n",
       "          6.02916240e-03,   7.10450461e-04,   2.83341629e-03,\n",
       "          4.02477568e-03,  -4.46277091e-03,   1.99333502e-03,\n",
       "         -2.55976260e-04,   1.62619806e-03,  -2.71433282e-03,\n",
       "         -1.89950208e-03,  -6.10084677e-03,   3.33742356e-04,\n",
       "          2.46145604e-03,  -7.63680899e-04,   6.19040028e-03,\n",
       "          1.37882685e-03,  -2.59925697e-05,  -8.87175588e-05,\n",
       "          1.81621368e-03,   1.03220412e-03,  -3.91430717e-03,\n",
       "          8.79272221e-05,   1.88558150e-03,  -1.94184903e-03,\n",
       "         -4.29366978e-03,   4.01950220e-03,   2.45259743e-03,\n",
       "          3.53506564e-04,  -1.63180219e-03,  -9.14496217e-05,\n",
       "         -1.54243044e-03,  -7.65757099e-04,   1.06540647e-03,\n",
       "          5.10275674e-04,   3.34802195e-04,  -3.56834680e-04,\n",
       "          1.11222242e-03,  -8.09990947e-05,  -2.90440552e-03,\n",
       "          2.36289246e-03,   2.05537715e-04,  -2.54968540e-03,\n",
       "          1.19765426e-03,   1.53880438e-03,  -9.93573870e-04,\n",
       "         -8.36801433e-04,   8.20765501e-04,   1.41183512e-03,\n",
       "         -5.02120650e-04,   8.92359504e-04,   8.11979347e-04,\n",
       "          4.98916098e-04,  -7.58420702e-04,   8.60490025e-04,\n",
       "          1.08321241e-03,   2.66340163e-04,   8.26838595e-04,\n",
       "          1.56624898e-03,  -2.57510469e-03,  -1.05886910e-03,\n",
       "         -1.81776925e-03,  -2.09785290e-03,   8.27252954e-04,\n",
       "          7.85400768e-04,  -9.01392803e-04,   3.43615068e-04,\n",
       "          6.80460519e-04,   1.99480889e-03,  -3.93409050e-05,\n",
       "         -4.72415369e-03,  -7.61787371e-04,  -1.03324465e-03,\n",
       "          1.01680946e-03,  -2.37920630e-04,   2.18302137e-03,\n",
       "          1.81882154e-03,   3.08059693e-04,   2.76069196e-04,\n",
       "          1.06323792e-03,   1.53428512e-03,   4.04310165e-04,\n",
       "          2.91430138e-04,   2.32721217e-03,  -7.22082494e-04,\n",
       "          1.89267812e-04,  -1.54985786e-03,   1.24667258e-03,\n",
       "         -4.73294907e-05,  -5.44377094e-03,  -3.39331618e-03,\n",
       "         -2.62906047e-03,  -3.04518026e-03,  -1.80315910e-03,\n",
       "         -2.42469109e-03,  -1.86880001e-03,  -2.12319204e-03,\n",
       "         -7.48641966e-04,  -3.98982494e-03,  -2.12217783e-03,\n",
       "         -1.78830130e-03,  -1.57263821e-03,  -1.35828369e-03,\n",
       "         -7.72818081e-04,  -1.39771685e-03,  -3.32555294e-03,\n",
       "         -3.70929691e-03,  -3.07656135e-04,  -7.20166007e-04,\n",
       "         -8.50567786e-04,  -5.78061703e-04,  -9.42324092e-04,\n",
       "         -6.08231989e-04,  -9.10034173e-04,  -1.58647716e-03,\n",
       "         -1.81501383e-03,  -2.58845136e-04,  -1.06531453e-03,\n",
       "         -1.82169865e-04,  -2.83883365e-04,  -2.41208636e-04,\n",
       "         -6.96602449e-04,  -1.28962786e-03,  -1.01146463e-03,\n",
       "         -1.19980910e-03,  -8.10021515e-04,  -2.04832961e-03,\n",
       "         -1.30562701e-03,  -3.45007000e-04,  -1.04662220e-03,\n",
       "         -7.00127282e-04,  -1.36703891e-03,  -1.74239785e-03,\n",
       "         -9.60601466e-04,  -1.46615633e-04,  -4.97617297e-06,\n",
       "          1.10261339e-04,   2.91662816e-04,  -1.04242715e-03,\n",
       "          1.90604448e-04,  -1.11075129e-03,  -6.02666075e-04,\n",
       "         -6.48565302e-04,  -2.47532364e-04,  -4.88683375e-04,\n",
       "          1.79969719e-04,   1.46119269e-05,  -4.65586802e-04,\n",
       "         -1.46601046e-03,  -1.57417777e-03,   2.20166274e-04,\n",
       "         -1.42892123e-03,  -1.40478558e-03,  -5.56184999e-04,\n",
       "          4.15309543e-04,  -6.28607998e-04,  -8.57403036e-04,\n",
       "         -1.03439406e-03,  -7.55911050e-04,  -1.70554770e-03,\n",
       "         -4.04245436e-04,  -2.21163478e-03,  -9.35189836e-04,\n",
       "         -7.52588697e-04,  -2.16836742e-03,   2.78983248e-04,\n",
       "         -1.29306877e-03,  -9.08899921e-04,  -1.61234880e-03,\n",
       "         -4.82303631e-04,  -9.30059596e-04,  -8.54853063e-04,\n",
       "         -9.89135509e-04,  -1.65963511e-03,  -4.27288259e-05,\n",
       "         -1.55427181e-03,   3.50358188e-04,  -1.13482767e-03,\n",
       "         -1.40708895e-03,   4.88876807e-04,  -7.73753893e-04,\n",
       "          3.83546998e-04,   1.75999321e-05,  -9.19119868e-04,\n",
       "         -1.05175055e-04,  -9.65290018e-04])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Compare coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare the coefficients for each of the models that were trained above. Create a table of features and learned coefficients associated with each of the different L2 penalty values.\n",
    "\n",
    "Using the coefficients trained with L2 penalty 0, find the 5 most positive words (with largest positive coefficients). Save them to positive_words. Similarly, find the 5 most negative words (with largest negative coefficients) and save them to negative_words.\n",
    "\n",
    "Quiz Question. Which of the following is not listed in either positive_words or negative_words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefficients_0_penalty = list(coefficients_dict['coefficients_0_penalty'][1:]) # exclude intercept\n",
    "word_coefficient_0_penalty_tuples = [(word, coefficient) for word, coefficient in zip(important_words, coefficients_0_penalty)]\n",
    "word_coefficient_0_penalty_tuples = sorted(word_coefficient_0_penalty_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.009111658707277601),\n",
       " ('great', 0.0090583410787265749),\n",
       " ('easy', 0.0088793180712080141),\n",
       " ('loves', 0.006190645867205467),\n",
       " ('little', 0.0060294019786029654)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_0_penalty_tuples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('get', -0.004462941344722764),\n",
       " ('work', -0.00472433795893599),\n",
       " ('money', -0.0054439850449472943),\n",
       " ('product', -0.0061010819064690608),\n",
       " ('would', -0.0079642474141327878)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_0_penalty_tuples[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe the effect of increasing L2 penalty on the 10 words just selected. Make a plot of the coefficients for the 10 words over the different values of L2 penalty.\n",
    "\n",
    "Hints:\n",
    "\n",
    "First, extract rows corresponding to positive_words. Do the same for negative_words.\n",
    "Then plot each of the extracted rows. The x axis should be L2 penalty and the y axis should be the coefficient value.\n",
    "Use log scale for the x axis, as the L2 penalty values are exponentially spaced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dataframe for important words and coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dict = dict(coefficients_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dict.update({'word': ['(intercept)'] + important_words})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_penality_coef = pd.DataFrame(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coefficients_0_penalty</th>\n",
       "      <th>coefficients_10_penalty</th>\n",
       "      <th>coefficients_1e2_penalty</th>\n",
       "      <th>coefficients_1e3_penalty</th>\n",
       "      <th>coefficients_1e5_penalty</th>\n",
       "      <th>coefficients_4_penalty</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>-0.104351</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>(intercept)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>-17.344002</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>baby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.001642</td>\n",
       "      <td>-0.001642</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.001628</td>\n",
       "      <td>-19.459873</td>\n",
       "      <td>-0.001642</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.009049</td>\n",
       "      <td>0.008970</td>\n",
       "      <td>-10.768619</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.009111</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>-6.845966</td>\n",
       "      <td>0.009111</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coefficients_0_penalty  coefficients_10_penalty  coefficients_1e2_penalty  \\\n",
       "0                0.000526                 0.000526                  0.000526   \n",
       "1                0.001949                 0.001949                  0.001947   \n",
       "2               -0.001642                -0.001642                 -0.001641   \n",
       "3                0.009058                 0.009057                  0.009049   \n",
       "4                0.009112                 0.009111                  0.009103   \n",
       "\n",
       "   coefficients_1e3_penalty  coefficients_1e5_penalty  coefficients_4_penalty  \\\n",
       "0                  0.000525                 -0.104351                0.000526   \n",
       "1                  0.001929                -17.344002                0.001949   \n",
       "2                 -0.001628                -19.459873               -0.001642   \n",
       "3                  0.008970                -10.768619                0.009058   \n",
       "4                  0.009023                 -6.845966                0.009111   \n",
       "\n",
       "          word  \n",
       "0  (intercept)  \n",
       "1         baby  \n",
       "2          one  \n",
       "3        great  \n",
       "4         love  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_penality_coef.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "newcolumns = word_penality_coef.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newcolumns_2 = newcolumns[-1:] + newcolumns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficients_0_penalty</th>\n",
       "      <th>coefficients_10_penalty</th>\n",
       "      <th>coefficients_1e2_penalty</th>\n",
       "      <th>coefficients_1e3_penalty</th>\n",
       "      <th>coefficients_1e5_penalty</th>\n",
       "      <th>coefficients_4_penalty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(intercept)</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>-0.104351</td>\n",
       "      <td>0.000526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baby</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>-17.344002</td>\n",
       "      <td>0.001949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>-0.001642</td>\n",
       "      <td>-0.001642</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>-0.001628</td>\n",
       "      <td>-19.459873</td>\n",
       "      <td>-0.001642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great</td>\n",
       "      <td>0.009058</td>\n",
       "      <td>0.009057</td>\n",
       "      <td>0.009049</td>\n",
       "      <td>0.008970</td>\n",
       "      <td>-10.768619</td>\n",
       "      <td>0.009058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>0.009112</td>\n",
       "      <td>0.009111</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.009023</td>\n",
       "      <td>-6.845966</td>\n",
       "      <td>0.009111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>use</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>-13.821567</td>\n",
       "      <td>0.000552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>would</td>\n",
       "      <td>-0.007964</td>\n",
       "      <td>-0.007963</td>\n",
       "      <td>-0.007957</td>\n",
       "      <td>-0.007889</td>\n",
       "      <td>-17.091981</td>\n",
       "      <td>-0.007964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-0.000695</td>\n",
       "      <td>-13.099545</td>\n",
       "      <td>-0.000701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.008878</td>\n",
       "      <td>0.008871</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>-7.242241</td>\n",
       "      <td>0.008879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>little</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.006029</td>\n",
       "      <td>0.006023</td>\n",
       "      <td>0.005970</td>\n",
       "      <td>-9.038187</td>\n",
       "      <td>0.006029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seat</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>-11.363187</td>\n",
       "      <td>0.000710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>old</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.002833</td>\n",
       "      <td>0.002831</td>\n",
       "      <td>0.002806</td>\n",
       "      <td>-9.301705</td>\n",
       "      <td>0.002833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>well</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.004021</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>-8.102610</td>\n",
       "      <td>0.004025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>get</td>\n",
       "      <td>-0.004463</td>\n",
       "      <td>-0.004463</td>\n",
       "      <td>-0.004459</td>\n",
       "      <td>-0.004421</td>\n",
       "      <td>-12.139269</td>\n",
       "      <td>-0.004463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>also</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.001993</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>-6.830683</td>\n",
       "      <td>0.001993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>really</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>-0.000254</td>\n",
       "      <td>-8.681495</td>\n",
       "      <td>-0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>son</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>-7.472645</td>\n",
       "      <td>0.001626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>-0.002712</td>\n",
       "      <td>-0.002689</td>\n",
       "      <td>-9.482146</td>\n",
       "      <td>-0.002714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bought</td>\n",
       "      <td>-0.001900</td>\n",
       "      <td>-0.001899</td>\n",
       "      <td>-0.001898</td>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-8.518076</td>\n",
       "      <td>-0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>product</td>\n",
       "      <td>-0.006101</td>\n",
       "      <td>-0.006100</td>\n",
       "      <td>-0.006095</td>\n",
       "      <td>-0.006043</td>\n",
       "      <td>-10.817459</td>\n",
       "      <td>-0.006101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>good</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000334</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>-6.778926</td>\n",
       "      <td>0.000334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>daughter</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>-6.031793</td>\n",
       "      <td>0.002461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>much</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>-0.000763</td>\n",
       "      <td>-0.000757</td>\n",
       "      <td>-7.715902</td>\n",
       "      <td>-0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>loves</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.006190</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>-3.834099</td>\n",
       "      <td>0.006190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stroller</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>-6.202112</td>\n",
       "      <td>0.001379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>put</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-7.350225</td>\n",
       "      <td>-0.000026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>months</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>-7.046027</td>\n",
       "      <td>-0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>car</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>-5.665026</td>\n",
       "      <td>0.001816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>still</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>-5.859155</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>back</td>\n",
       "      <td>-0.003914</td>\n",
       "      <td>-0.003914</td>\n",
       "      <td>-0.003911</td>\n",
       "      <td>-0.003877</td>\n",
       "      <td>-7.928242</td>\n",
       "      <td>-0.003914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>started</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-0.000857</td>\n",
       "      <td>-0.000849</td>\n",
       "      <td>-1.979327</td>\n",
       "      <td>-0.000857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>anything</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.001034</td>\n",
       "      <td>-0.001033</td>\n",
       "      <td>-0.001024</td>\n",
       "      <td>-1.885217</td>\n",
       "      <td>-0.001034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>last</td>\n",
       "      <td>-0.000756</td>\n",
       "      <td>-0.000756</td>\n",
       "      <td>-0.000755</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-1.964874</td>\n",
       "      <td>-0.000756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>company</td>\n",
       "      <td>-0.001706</td>\n",
       "      <td>-0.001705</td>\n",
       "      <td>-0.001704</td>\n",
       "      <td>-0.001689</td>\n",
       "      <td>-1.413277</td>\n",
       "      <td>-0.001706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>come</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.000404</td>\n",
       "      <td>-0.000400</td>\n",
       "      <td>-2.196680</td>\n",
       "      <td>-0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>returned</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.002190</td>\n",
       "      <td>-1.246329</td>\n",
       "      <td>-0.002212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>maybe</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.000935</td>\n",
       "      <td>-0.000934</td>\n",
       "      <td>-0.000926</td>\n",
       "      <td>-1.040301</td>\n",
       "      <td>-0.000935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>took</td>\n",
       "      <td>-0.000753</td>\n",
       "      <td>-0.000753</td>\n",
       "      <td>-0.000752</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-2.035988</td>\n",
       "      <td>-0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>broke</td>\n",
       "      <td>-0.002168</td>\n",
       "      <td>-0.002168</td>\n",
       "      <td>-0.002166</td>\n",
       "      <td>-0.002147</td>\n",
       "      <td>-1.351660</td>\n",
       "      <td>-0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>makes</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>-2.422716</td>\n",
       "      <td>0.000279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>stay</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>-0.001281</td>\n",
       "      <td>-1.657250</td>\n",
       "      <td>-0.001293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>instead</td>\n",
       "      <td>-0.000909</td>\n",
       "      <td>-0.000909</td>\n",
       "      <td>-0.000908</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>-1.629811</td>\n",
       "      <td>-0.000909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>idea</td>\n",
       "      <td>-0.001612</td>\n",
       "      <td>-0.001612</td>\n",
       "      <td>-0.001611</td>\n",
       "      <td>-0.001597</td>\n",
       "      <td>-1.488311</td>\n",
       "      <td>-0.001612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>head</td>\n",
       "      <td>-0.000482</td>\n",
       "      <td>-0.000482</td>\n",
       "      <td>-0.000482</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>-2.062254</td>\n",
       "      <td>-0.000482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>said</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>-0.000929</td>\n",
       "      <td>-0.000921</td>\n",
       "      <td>-1.801849</td>\n",
       "      <td>-0.000930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>less</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>-0.000854</td>\n",
       "      <td>-0.000847</td>\n",
       "      <td>-1.855023</td>\n",
       "      <td>-0.000855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>went</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.000989</td>\n",
       "      <td>-0.000988</td>\n",
       "      <td>-0.000980</td>\n",
       "      <td>-1.790612</td>\n",
       "      <td>-0.000989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>working</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.001658</td>\n",
       "      <td>-0.001644</td>\n",
       "      <td>-1.437500</td>\n",
       "      <td>-0.001660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>high</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>-2.044657</td>\n",
       "      <td>-0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>unit</td>\n",
       "      <td>-0.001554</td>\n",
       "      <td>-0.001554</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>-0.001539</td>\n",
       "      <td>-1.358695</td>\n",
       "      <td>-0.001554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>seems</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>-2.260900</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>picture</td>\n",
       "      <td>-0.001135</td>\n",
       "      <td>-0.001135</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>-0.001124</td>\n",
       "      <td>-1.565070</td>\n",
       "      <td>-0.001135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>completely</td>\n",
       "      <td>-0.001407</td>\n",
       "      <td>-0.001407</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>-0.001393</td>\n",
       "      <td>-1.449027</td>\n",
       "      <td>-0.001407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>wish</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>-2.100392</td>\n",
       "      <td>0.000489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>buying</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>-0.000774</td>\n",
       "      <td>-0.000773</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>-1.776960</td>\n",
       "      <td>-0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>babies</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-1.911529</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>won</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.024602</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tub</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>-0.000918</td>\n",
       "      <td>-0.000910</td>\n",
       "      <td>-1.669593</td>\n",
       "      <td>-0.000919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>almost</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>-2.015657</td>\n",
       "      <td>-0.000105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>either</td>\n",
       "      <td>-0.000965</td>\n",
       "      <td>-0.000965</td>\n",
       "      <td>-0.000964</td>\n",
       "      <td>-0.000956</td>\n",
       "      <td>-1.505660</td>\n",
       "      <td>-0.000965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  coefficients_0_penalty  coefficients_10_penalty  \\\n",
       "0    (intercept)                0.000526                 0.000526   \n",
       "1           baby                0.001949                 0.001949   \n",
       "2            one               -0.001642                -0.001642   \n",
       "3          great                0.009058                 0.009057   \n",
       "4           love                0.009112                 0.009111   \n",
       "5            use                0.000552                 0.000552   \n",
       "6          would               -0.007964                -0.007963   \n",
       "7           like               -0.000701                -0.000701   \n",
       "8           easy                0.008879                 0.008878   \n",
       "9         little                0.006029                 0.006029   \n",
       "10          seat                0.000710                 0.000710   \n",
       "11           old                0.002834                 0.002833   \n",
       "12          well                0.004025                 0.004025   \n",
       "13           get               -0.004463                -0.004463   \n",
       "14          also                0.001993                 0.001993   \n",
       "15        really               -0.000256                -0.000256   \n",
       "16           son                0.001626                 0.001626   \n",
       "17          time               -0.002714                -0.002714   \n",
       "18        bought               -0.001900                -0.001899   \n",
       "19       product               -0.006101                -0.006100   \n",
       "20          good                0.000334                 0.000334   \n",
       "21      daughter                0.002462                 0.002461   \n",
       "22          much               -0.000764                -0.000764   \n",
       "23         loves                0.006191                 0.006190   \n",
       "24      stroller                0.001379                 0.001379   \n",
       "25           put               -0.000026                -0.000026   \n",
       "26        months               -0.000089                -0.000089   \n",
       "27           car                0.001816                 0.001816   \n",
       "28         still                0.001032                 0.001032   \n",
       "29          back               -0.003914                -0.003914   \n",
       "..           ...                     ...                      ...   \n",
       "164      started               -0.000857                -0.000857   \n",
       "165     anything               -0.001034                -0.001034   \n",
       "166         last               -0.000756                -0.000756   \n",
       "167      company               -0.001706                -0.001705   \n",
       "168         come               -0.000404                -0.000404   \n",
       "169     returned               -0.002212                -0.002212   \n",
       "170        maybe               -0.000935                -0.000935   \n",
       "171         took               -0.000753                -0.000753   \n",
       "172        broke               -0.002168                -0.002168   \n",
       "173        makes                0.000279                 0.000279   \n",
       "174         stay               -0.001293                -0.001293   \n",
       "175      instead               -0.000909                -0.000909   \n",
       "176         idea               -0.001612                -0.001612   \n",
       "177         head               -0.000482                -0.000482   \n",
       "178         said               -0.000930                -0.000930   \n",
       "179         less               -0.000855                -0.000855   \n",
       "180         went               -0.000989                -0.000989   \n",
       "181      working               -0.001660                -0.001660   \n",
       "182         high               -0.000043                -0.000043   \n",
       "183         unit               -0.001554                -0.001554   \n",
       "184        seems                0.000350                 0.000350   \n",
       "185      picture               -0.001135                -0.001135   \n",
       "186   completely               -0.001407                -0.001407   \n",
       "187         wish                0.000489                 0.000489   \n",
       "188       buying               -0.000774                -0.000774   \n",
       "189       babies                0.000384                 0.000384   \n",
       "190          won                0.000018                 0.000018   \n",
       "191          tub               -0.000919                -0.000919   \n",
       "192       almost               -0.000105                -0.000105   \n",
       "193       either               -0.000965                -0.000965   \n",
       "\n",
       "     coefficients_1e2_penalty  coefficients_1e3_penalty  \\\n",
       "0                    0.000526                  0.000525   \n",
       "1                    0.001947                  0.001929   \n",
       "2                   -0.001641                 -0.001628   \n",
       "3                    0.009049                  0.008970   \n",
       "4                    0.009103                  0.009023   \n",
       "5                    0.000551                  0.000546   \n",
       "6                   -0.007957                 -0.007889   \n",
       "7                   -0.000701                 -0.000695   \n",
       "8                    0.008871                  0.008792   \n",
       "9                    0.006023                  0.005970   \n",
       "10                   0.000710                  0.000703   \n",
       "11                   0.002831                  0.002806   \n",
       "12                   0.004021                  0.003985   \n",
       "13                  -0.004459                 -0.004421   \n",
       "14                   0.001991                  0.001974   \n",
       "15                  -0.000256                 -0.000254   \n",
       "16                   0.001625                  0.001610   \n",
       "17                  -0.002712                 -0.002689   \n",
       "18                  -0.001898                 -0.001882   \n",
       "19                  -0.006095                 -0.006043   \n",
       "20                   0.000333                  0.000330   \n",
       "21                   0.002459                  0.002437   \n",
       "22                  -0.000763                 -0.000757   \n",
       "23                   0.006185                  0.006130   \n",
       "24                   0.001378                  0.001366   \n",
       "25                  -0.000026                 -0.000026   \n",
       "26                  -0.000089                 -0.000088   \n",
       "27                   0.001814                  0.001798   \n",
       "28                   0.001031                  0.001022   \n",
       "29                  -0.003911                 -0.003877   \n",
       "..                        ...                       ...   \n",
       "164                 -0.000857                 -0.000849   \n",
       "165                 -0.001033                 -0.001024   \n",
       "166                 -0.000755                 -0.000749   \n",
       "167                 -0.001704                 -0.001689   \n",
       "168                 -0.000404                 -0.000400   \n",
       "169                 -0.002210                 -0.002190   \n",
       "170                 -0.000934                 -0.000926   \n",
       "171                 -0.000752                 -0.000745   \n",
       "172                 -0.002166                 -0.002147   \n",
       "173                  0.000279                  0.000276   \n",
       "174                 -0.001292                 -0.001281   \n",
       "175                 -0.000908                 -0.000900   \n",
       "176                 -0.001611                 -0.001597   \n",
       "177                 -0.000482                 -0.000478   \n",
       "178                 -0.000929                 -0.000921   \n",
       "179                 -0.000854                 -0.000847   \n",
       "180                 -0.000988                 -0.000980   \n",
       "181                 -0.001658                 -0.001644   \n",
       "182                 -0.000043                 -0.000042   \n",
       "183                 -0.001553                 -0.001539   \n",
       "184                  0.000350                  0.000347   \n",
       "185                 -0.001134                 -0.001124   \n",
       "186                 -0.001406                 -0.001393   \n",
       "187                  0.000488                  0.000484   \n",
       "188                 -0.000773                 -0.000766   \n",
       "189                  0.000383                  0.000380   \n",
       "190                  0.000018                  0.000017   \n",
       "191                 -0.000918                 -0.000910   \n",
       "192                 -0.000105                 -0.000104   \n",
       "193                 -0.000964                 -0.000956   \n",
       "\n",
       "     coefficients_1e5_penalty  coefficients_4_penalty  \n",
       "0                   -0.104351                0.000526  \n",
       "1                  -17.344002                0.001949  \n",
       "2                  -19.459873               -0.001642  \n",
       "3                  -10.768619                0.009058  \n",
       "4                   -6.845966                0.009111  \n",
       "5                  -13.821567                0.000552  \n",
       "6                  -17.091981               -0.007964  \n",
       "7                  -13.099545               -0.000701  \n",
       "8                   -7.242241                0.008879  \n",
       "9                   -9.038187                0.006029  \n",
       "10                 -11.363187                0.000710  \n",
       "11                  -9.301705                0.002833  \n",
       "12                  -8.102610                0.004025  \n",
       "13                 -12.139269               -0.004463  \n",
       "14                  -6.830683                0.001993  \n",
       "15                  -8.681495               -0.000256  \n",
       "16                  -7.472645                0.001626  \n",
       "17                  -9.482146               -0.002714  \n",
       "18                  -8.518076               -0.001900  \n",
       "19                 -10.817459               -0.006101  \n",
       "20                  -6.778926                0.000334  \n",
       "21                  -6.031793                0.002461  \n",
       "22                  -7.715902               -0.000764  \n",
       "23                  -3.834099                0.006190  \n",
       "24                  -6.202112                0.001379  \n",
       "25                  -7.350225               -0.000026  \n",
       "26                  -7.046027               -0.000089  \n",
       "27                  -5.665026                0.001816  \n",
       "28                  -5.859155                0.001032  \n",
       "29                  -7.928242               -0.003914  \n",
       "..                        ...                     ...  \n",
       "164                 -1.979327               -0.000857  \n",
       "165                 -1.885217               -0.001034  \n",
       "166                 -1.964874               -0.000756  \n",
       "167                 -1.413277               -0.001706  \n",
       "168                 -2.196680               -0.000404  \n",
       "169                 -1.246329               -0.002212  \n",
       "170                 -1.040301               -0.000935  \n",
       "171                 -2.035988               -0.000753  \n",
       "172                 -1.351660               -0.002168  \n",
       "173                 -2.422716                0.000279  \n",
       "174                 -1.657250               -0.001293  \n",
       "175                 -1.629811               -0.000909  \n",
       "176                 -1.488311               -0.001612  \n",
       "177                 -2.062254               -0.000482  \n",
       "178                 -1.801849               -0.000930  \n",
       "179                 -1.855023               -0.000855  \n",
       "180                 -1.790612               -0.000989  \n",
       "181                 -1.437500               -0.001660  \n",
       "182                 -2.044657               -0.000043  \n",
       "183                 -1.358695               -0.001554  \n",
       "184                 -2.260900                0.000350  \n",
       "185                 -1.565070               -0.001135  \n",
       "186                 -1.449027               -0.001407  \n",
       "187                 -2.100392                0.000489  \n",
       "188                 -1.776960               -0.000774  \n",
       "189                 -1.911529                0.000384  \n",
       "190                 -0.024602                0.000018  \n",
       "191                 -1.669593               -0.000919  \n",
       "192                 -2.015657               -0.000105  \n",
       "193                 -1.505660               -0.000965  \n",
       "\n",
       "[194 rows x 7 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_penality_coef = word_penality_coef[newcolumns_2]\n",
    "word_penality_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "def make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list):\n",
    "    cmap_positive = plt.get_cmap('Reds')\n",
    "    cmap_negative = plt.get_cmap('Blues')\n",
    "    \n",
    "    xx = l2_penalty_list\n",
    "    plt.plot(xx, [0.]*len(xx), '--', lw=1, color='k')\n",
    "    \n",
    "    table_positive_words = table[table['word'].isin(positive_words)]\n",
    "    table_negative_words = table[table['word'].isin(negative_words)]\n",
    "    del table_positive_words['word']\n",
    "    del table_negative_words['word']\n",
    "    \n",
    "    for i in xrange(len(positive_words)):\n",
    "        color = cmap_positive(0.8*((i+1)/(len(positive_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_positive_words[i:i+1].as_matrix().flatten(),\n",
    "                 '-', label=positive_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    for i in xrange(len(negative_words)):\n",
    "        color = cmap_negative(0.8*((i+1)/(len(negative_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_negative_words[i:i+1].as_matrix().flatten(),\n",
    "                 '-', label=negative_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    plt.legend(loc='best', ncol=3, prop={'size':16}, columnspacing=0.5)\n",
    "    plt.axis([1, 1e5, -1, 2])\n",
    "    plt.title('Coefficient path')\n",
    "    plt.xlabel('L2 penalty ($\\lambda$)')\n",
    "    plt.ylabel('Coefficient value')\n",
    "    plt.xscale('log')\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_words = [pw for pw, coef in word_coefficient_0_penalty_tuples[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_words = [nw for nw, coeff in word_coefficient_0_penalty_tuples[-5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get', 'work', 'money', 'product', 'would']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArgAAAGYCAYAAACphJGEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1099f6710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_coefficient_plot(word_penality_coef, positive_words, negative_words, l2_penalty_list=[0, 4, 10, 1e2, 1e3, 1e5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Measuring accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: Which model (L2 = 0, 4, 10, 100, 1e3, 1e5) has the highest accuracy on the training data?\n",
    "Quiz question: Which model (L2 = 0, 4, 10, 100, 1e3, 1e5) has the highest accuracy on the validation data?\n",
    "Quiz question: Does the highest accuracy on the training data imply that the model is the best one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_accuaracy(feature_matrix, sentiment, coefficients):\n",
    "    scores_new = np.dot(feature_matrix, coefficients)\n",
    "    predictions = np.array([+1 if s > 0 else -1 for s in scores_new])\n",
    "    num_correct = sum(predictions == sentiment)\n",
    "    accuracy = float(num_correct) / len(feature_matrix)   \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_accuracy = {}\n",
    "train_accuracy[0]   = model_accuaracy(feature_matrix_train, sentiment_train, coefficients_dict['coefficients_0_penalty'])\n",
    "train_accuracy[4]   = model_accuaracy(feature_matrix_train, sentiment_train, coefficients_dict['coefficients_4_penalty'])\n",
    "train_accuracy[10]  = model_accuaracy(feature_matrix_train, sentiment_train, coefficients_dict['coefficients_10_penalty'])\n",
    "train_accuracy[1e2] = model_accuaracy(feature_matrix_train, sentiment_train, coefficients_dict['coefficients_1e2_penalty'])\n",
    "train_accuracy[1e3] = model_accuaracy(feature_matrix_train, sentiment_train, coefficients_dict['coefficients_1e3_penalty'])\n",
    "train_accuracy[1e5] = model_accuaracy(feature_matrix_train, sentiment_train, coefficients_dict['coefficients_1e5_penalty'])\n",
    "\n",
    "validation_accuracy = {}\n",
    "validation_accuracy[0]   = model_accuaracy(feature_matrix_valid, sentiment_valid, coefficients_dict['coefficients_0_penalty'])\n",
    "validation_accuracy[4]   = model_accuaracy(feature_matrix_valid, sentiment_valid, coefficients_dict['coefficients_4_penalty'])\n",
    "validation_accuracy[10]  = model_accuaracy(feature_matrix_valid, sentiment_valid, coefficients_dict['coefficients_10_penalty'])\n",
    "validation_accuracy[1e2] = model_accuaracy(feature_matrix_valid, sentiment_valid, coefficients_dict['coefficients_1e2_penalty'])\n",
    "validation_accuracy[1e3] = model_accuaracy(feature_matrix_valid, sentiment_valid, coefficients_dict['coefficients_1e3_penalty'])\n",
    "validation_accuracy[1e5] = model_accuaracy(feature_matrix_valid, sentiment_valid, coefficients_dict['coefficients_1e5_penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 penalty = 0\n",
      "train accuracy = 0.747574419867, validation_accuracy = 0.746522266828\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 4\n",
      "train accuracy = 0.747574419867, validation_accuracy = 0.746522266828\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 10\n",
      "train accuracy = 0.747574419867, validation_accuracy = 0.746522266828\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100\n",
      "train accuracy = 0.747574419867, validation_accuracy = 0.746522266828\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 1000\n",
      "train accuracy = 0.747503600009, validation_accuracy = 0.746615628793\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100000\n",
      "train accuracy = 0.498430159817, validation_accuracy = 0.502194006162\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build a simple report\n",
    "for key in sorted(validation_accuracy.keys()):\n",
    "    print \"L2 penalty = %g\" % key\n",
    "    print \"train accuracy = %s, validation_accuracy = %s\" % (train_accuracy[key], validation_accuracy[key])\n",
    "    print \"--------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
